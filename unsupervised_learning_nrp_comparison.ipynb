{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yohnjparra/test_unsupervisedlearning/blob/main/unsupervised_learning_nrp_comparison.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "By6rzNPftXc_"
      },
      "source": [
        "# Unsupervised Learning: Performance Comparison Notebook\n",
        "\n",
        "**Course:** Machine Learning | **Instructor:** Dr. Yohn  \n",
        "**Institution:** Florida A&M University - Computer and Information Sciences Department\n",
        "\n",
        "## Objective\n",
        "This notebook demonstrates unsupervised learning algorithms while comparing computational performance between **JupyterHub NRP** and **Google Colab** environments. Students will:\n",
        "\n",
        "1. Learn key unsupervised learning algorithms (K-Means, DBSCAN, Hierarchical Clustering)\n",
        "2. Understand computational resource considerations\n",
        "3. Compare performance metrics across different environments\n",
        "4. Evaluate clustering quality using multiple metrics\n",
        "\n",
        "## Instructions\n",
        "1. Run this notebook completely on **JupyterHub NRP**\n",
        "2. Save the generated CSV and images\n",
        "3. Run the same notebook on **Google Colab**\n",
        "4. Compare the results to understand infrastructure differences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zC1RBE9DtXdA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff39ba8a-7077-4728-b49b-6c56f5901a41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "UNSUPERVISED LEARNING: Performance Comparison Notebook\n",
            "Course: Machine Learning | Instructor: Dr. Yohn\n",
            "Florida A&M University - CIS Department\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# Cell 1: Import Libraries and Setup\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import make_blobs, make_classification\n",
        "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
        "import psutil\n",
        "import platform\n",
        "\n",
        "# Set style for visualizations\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"UNSUPERVISED LEARNING: Performance Comparison Notebook\")\n",
        "print(\"Course: Machine Learning | Instructor: Dr. Yohn\")\n",
        "print(\"Florida A&M University - CIS Department\")\n",
        "print(\"=\" * 80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "WTYMdsL4tXdA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de55376a-759d-409b-bbf9-5c904d2b2df4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä SYSTEM INFORMATION\n",
            "============================================================\n",
            "            Property                               Value\n",
            "            Platform                               Linux\n",
            "    Platform Release                            6.6.105+\n",
            "    Platform Version #1 SMP Thu Oct  2 10:42:05 UTC 2025\n",
            "        Architecture                              x86_64\n",
            "           Processor                              x86_64\n",
            "CPU Cores (Physical)                                   1\n",
            " CPU Cores (Logical)                                   2\n",
            "      Total RAM (GB)                               12.67\n",
            "  Available RAM (GB)                               11.15\n",
            "      Python Version                             3.12.12\n",
            "         Environment                        Google Colab\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# Cell 2: System Information and Resource Detection\n",
        "def get_system_info():\n",
        "    \"\"\"\n",
        "    Display system information to compare computational resources\n",
        "    between NRP JupyterHub and Google Colab\n",
        "    \"\"\"\n",
        "    info = {\n",
        "        'Platform': platform.system(),\n",
        "        'Platform Release': platform.release(),\n",
        "        'Platform Version': platform.version(),\n",
        "        'Architecture': platform.machine(),\n",
        "        'Processor': platform.processor(),\n",
        "        'CPU Cores (Physical)': psutil.cpu_count(logical=False),\n",
        "        'CPU Cores (Logical)': psutil.cpu_count(logical=True),\n",
        "        'Total RAM (GB)': round(psutil.virtual_memory().total / (1024**3), 2),\n",
        "        'Available RAM (GB)': round(psutil.virtual_memory().available / (1024**3), 2),\n",
        "        'Python Version': platform.python_version()\n",
        "    }\n",
        "\n",
        "    # Check if running on Google Colab\n",
        "    try:\n",
        "        import google.colab\n",
        "        info['Environment'] = 'Google Colab'\n",
        "    except:\n",
        "        info['Environment'] = 'JupyterHub NRP (or Local)'\n",
        "\n",
        "    # Display as DataFrame\n",
        "    df_info = pd.DataFrame(list(info.items()), columns=['Property', 'Value'])\n",
        "    print(\"\\nüìä SYSTEM INFORMATION\")\n",
        "    print(\"=\" * 60)\n",
        "    print(df_info.to_string(index=False))\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    return info\n",
        "\n",
        "system_info = get_system_info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "p1nKXVwZtXdB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1373f832-fada-499c-f2ea-6695cbc8b668"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üî¨ GENERATING DATASETS\n",
            "============================================================\n",
            "‚úì Small dataset: (1000, 10)\n",
            "‚úì Medium dataset: (50000, 20)\n",
            "‚úì Large dataset: (200000, 30)\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# Cell 3: Generate Synthetic Datasets of Various Sizes\n",
        "def create_datasets():\n",
        "    \"\"\"\n",
        "    Create datasets of different sizes to benchmark performance\n",
        "    \"\"\"\n",
        "    datasets = {}\n",
        "\n",
        "    print(\"\\nüî¨ GENERATING DATASETS\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Small dataset - for quick testing\n",
        "    X_small, y_small = make_blobs(n_samples=1000, n_features=10,\n",
        "                                   centers=5, random_state=42)\n",
        "    datasets['small'] = (X_small, y_small, \"Small (1K samples, 10 features)\")\n",
        "    print(f\"‚úì Small dataset: {X_small.shape}\")\n",
        "\n",
        "    # Medium dataset - typical classroom size\n",
        "    X_medium, y_medium = make_blobs(n_samples=50000, n_features=20,\n",
        "                                     centers=8, random_state=42)\n",
        "    datasets['medium'] = (X_medium, y_medium, \"Medium (50K samples, 20 features)\")\n",
        "    print(f\"‚úì Medium dataset: {X_medium.shape}\")\n",
        "\n",
        "    # Large dataset - to stress test resources\n",
        "    X_large, y_large = make_blobs(n_samples=200000, n_features=30,\n",
        "                                   centers=10, random_state=42)\n",
        "    datasets['large'] = (X_large, y_large, \"Large (200K samples, 30 features)\")\n",
        "    print(f\"‚úì Large dataset: {X_large.shape}\")\n",
        "\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    return datasets\n",
        "\n",
        "datasets = create_datasets()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Y308O86jtXdB"
      },
      "outputs": [],
      "source": [
        "# Cell 4: Performance Benchmarking Class\n",
        "class PerformanceBenchmark:\n",
        "    \"\"\"\n",
        "    Class to track and compare algorithm performance\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.results = []\n",
        "\n",
        "    def benchmark_algorithm(self, algorithm, X, dataset_name, algorithm_name):\n",
        "        \"\"\"\n",
        "        Benchmark a clustering algorithm\n",
        "        \"\"\"\n",
        "        # Record start metrics\n",
        "        start_time = time.time()\n",
        "        start_memory = psutil.Process().memory_info().rss / (1024**2)  # MB\n",
        "        start_cpu = psutil.cpu_percent(interval=0.1)\n",
        "\n",
        "        # Run algorithm\n",
        "        try:\n",
        "            labels = algorithm.fit_predict(X)\n",
        "\n",
        "            # Record end metrics\n",
        "            end_time = time.time()\n",
        "            end_memory = psutil.Process().memory_info().rss / (1024**2)  # MB\n",
        "            end_cpu = psutil.cpu_percent(interval=0.1)\n",
        "\n",
        "            # Calculate metrics\n",
        "            execution_time = end_time - start_time\n",
        "            memory_used = end_memory - start_memory\n",
        "            avg_cpu = (start_cpu + end_cpu) / 2\n",
        "\n",
        "            # Clustering quality metrics\n",
        "            silhouette = silhouette_score(X, labels) if len(np.unique(labels)) > 1 else 0\n",
        "            davies_bouldin = davies_bouldin_score(X, labels) if len(np.unique(labels)) > 1 else 0\n",
        "            calinski = calinski_harabasz_score(X, labels) if len(np.unique(labels)) > 1 else 0\n",
        "\n",
        "            result = {\n",
        "                'Dataset': dataset_name,\n",
        "                'Algorithm': algorithm_name,\n",
        "                'Samples': X.shape[0],\n",
        "                'Features': X.shape[1],\n",
        "                'Execution Time (s)': round(execution_time, 4),\n",
        "                'Memory Used (MB)': round(memory_used, 2),\n",
        "                'Avg CPU (%)': round(avg_cpu, 2),\n",
        "                'Silhouette Score': round(silhouette, 4),\n",
        "                'Davies-Bouldin Index': round(davies_bouldin, 4),\n",
        "                'Calinski-Harabasz Score': round(calinski, 2),\n",
        "                'N Clusters': len(np.unique(labels)),\n",
        "                'Status': 'Success'\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            result = {\n",
        "                'Dataset': dataset_name,\n",
        "                'Algorithm': algorithm_name,\n",
        "                'Samples': X.shape[0],\n",
        "                'Features': X.shape[1],\n",
        "                'Execution Time (s)': 0,\n",
        "                'Memory Used (MB)': 0,\n",
        "                'Avg CPU (%)': 0,\n",
        "                'Silhouette Score': 0,\n",
        "                'Davies-Bouldin Index': 0,\n",
        "                'Calinski-Harabasz Score': 0,\n",
        "                'N Clusters': 0,\n",
        "                'Status': f'Failed: {str(e)}'\n",
        "            }\n",
        "\n",
        "        self.results.append(result)\n",
        "        return result\n",
        "\n",
        "    def get_results_df(self):\n",
        "        \"\"\"Return results as a DataFrame\"\"\"\n",
        "        return pd.DataFrame(self.results)\n",
        "\n",
        "    def display_summary(self):\n",
        "        \"\"\"Display summary of results\"\"\"\n",
        "        df = self.get_results_df()\n",
        "        print(\"\\nüìà PERFORMANCE SUMMARY\")\n",
        "        print(\"=\" * 120)\n",
        "        print(df.to_string(index=False))\n",
        "        print(\"=\" * 120)\n",
        "        return df\n",
        "\n",
        "benchmark = PerformanceBenchmark()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mHJRfM-GtXdB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8f7d247-417e-44be-cc6e-c58da49f5b43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üöÄ RUNNING UNSUPERVISED LEARNING ALGORITHMS\n",
            "================================================================================\n",
            "\n",
            "üìä Processing Small (1K samples, 10 features)...\n",
            "  ‚è±Ô∏è  Running K-Means (k=5)... ‚úì (0.1596s)\n",
            "  ‚è±Ô∏è  Running K-Means (k=8)... ‚úì (0.1316s)\n",
            "  ‚è±Ô∏è  Running DBSCAN... ‚úì (0.1368s)\n",
            "  ‚è±Ô∏è  Running Hierarchical... ‚úì (0.1259s)\n",
            "\n",
            "üìä Processing Medium (50K samples, 20 features)...\n",
            "  ‚è±Ô∏è  Running K-Means (k=5)... ‚úì (0.8981s)\n",
            "  ‚è±Ô∏è  Running K-Means (k=8)... ‚úì (0.4719s)\n",
            "  ‚è±Ô∏è  Running DBSCAN... ‚úì (19.7166s)\n",
            "  ‚è±Ô∏è  Running Hierarchical... "
          ]
        }
      ],
      "source": [
        "# Cell 5: Run Clustering Algorithms on All Datasets\n",
        "print(\"\\nüöÄ RUNNING UNSUPERVISED LEARNING ALGORITHMS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "algorithms = {\n",
        "    'K-Means (k=5)': KMeans(n_clusters=5, random_state=42, n_init=10),\n",
        "    'K-Means (k=8)': KMeans(n_clusters=8, random_state=42, n_init=10),\n",
        "    'DBSCAN': DBSCAN(eps=3, min_samples=5),\n",
        "    'Hierarchical': AgglomerativeClustering(n_clusters=5)\n",
        "}\n",
        "\n",
        "for dataset_name, (X, y_true, description) in datasets.items():\n",
        "    print(f\"\\nüìä Processing {description}...\")\n",
        "\n",
        "    # Standardize features\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "    for algo_name, algorithm in algorithms.items():\n",
        "        print(f\"  ‚è±Ô∏è  Running {algo_name}...\", end=\" \")\n",
        "        result = benchmark.benchmark_algorithm(algorithm, X_scaled, dataset_name, algo_name)\n",
        "        print(f\"‚úì ({result['Execution Time (s)']}s)\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"‚úÖ ALL BENCHMARKS COMPLETED\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "UxNwixvOtXdB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "outputId": "4c3e8509-fd40-4f46-d837-4cf01407c978"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìà PERFORMANCE SUMMARY\n",
            "========================================================================================================================\n",
            "Empty DataFrame\n",
            "Columns: []\n",
            "Index: []\n",
            "========================================================================================================================\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'system_info' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3770635413.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Save results to CSV for later comparison\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0moutput_filename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"clustering_benchmark_{system_info['Environment'].replace(' ', '_')}.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mresults_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nüíæ Results saved to: {output_filename}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'system_info' is not defined"
          ]
        }
      ],
      "source": [
        "# Cell 6: Display Results and Save\n",
        "results_df = benchmark.display_summary()\n",
        "\n",
        "# Save results to CSV for later comparison\n",
        "output_filename = f\"clustering_benchmark_{system_info['Environment'].replace(' ', '_')}.csv\"\n",
        "results_df.to_csv(output_filename, index=False)\n",
        "print(f\"\\nüíæ Results saved to: {output_filename}\")\n",
        "print(\"üìå You can compare this file with results from Google Colab!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "piR-n4b7tXdB"
      },
      "outputs": [],
      "source": [
        "# Cell 7: Comprehensive Visualizations\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "# Plot 1: Execution Time by Dataset and Algorithm\n",
        "ax1 = axes[0, 0]\n",
        "pivot_time = results_df.pivot(index='Algorithm', columns='Dataset', values='Execution Time (s)')\n",
        "pivot_time.plot(kind='bar', ax=ax1, colormap='viridis')\n",
        "ax1.set_title('Execution Time Comparison Across Datasets', fontsize=14, fontweight='bold')\n",
        "ax1.set_ylabel('Time (seconds)', fontsize=12)\n",
        "ax1.set_xlabel('Algorithm', fontsize=12)\n",
        "ax1.legend(title='Dataset Size')\n",
        "ax1.grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Plot 2: Memory Usage Comparison\n",
        "ax2 = axes[0, 1]\n",
        "pivot_memory = results_df.pivot(index='Algorithm', columns='Dataset', values='Memory Used (MB)')\n",
        "pivot_memory.plot(kind='bar', ax=ax2, colormap='plasma')\n",
        "ax2.set_title('Memory Usage Comparison', fontsize=14, fontweight='bold')\n",
        "ax2.set_ylabel('Memory (MB)', fontsize=12)\n",
        "ax2.set_xlabel('Algorithm', fontsize=12)\n",
        "ax2.legend(title='Dataset Size')\n",
        "ax2.grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Plot 3: Clustering Quality - Silhouette Score\n",
        "ax3 = axes[1, 0]\n",
        "pivot_silhouette = results_df.pivot(index='Algorithm', columns='Dataset', values='Silhouette Score')\n",
        "pivot_silhouette.plot(kind='bar', ax=ax3, colormap='coolwarm')\n",
        "ax3.set_title('Clustering Quality: Silhouette Score (Higher is Better)', fontsize=14, fontweight='bold')\n",
        "ax3.set_ylabel('Silhouette Score', fontsize=12)\n",
        "ax3.set_xlabel('Algorithm', fontsize=12)\n",
        "ax3.legend(title='Dataset Size')\n",
        "ax3.axhline(y=0.5, color='red', linestyle='--', label='Good clustering threshold')\n",
        "ax3.grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Plot 4: Scalability Analysis\n",
        "ax4 = axes[1, 1]\n",
        "for algo in results_df['Algorithm'].unique():\n",
        "    algo_data = results_df[results_df['Algorithm'] == algo].sort_values('Samples')\n",
        "    ax4.plot(algo_data['Samples'], algo_data['Execution Time (s)'],\n",
        "             marker='o', label=algo, linewidth=2, markersize=8)\n",
        "ax4.set_title('Algorithm Scalability Analysis', fontsize=14, fontweight='bold')\n",
        "ax4.set_xlabel('Number of Samples', fontsize=12)\n",
        "ax4.set_ylabel('Execution Time (seconds)', fontsize=12)\n",
        "ax4.legend(loc='best')\n",
        "ax4.grid(True, alpha=0.3)\n",
        "ax4.set_xscale('log')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f'clustering_performance_{system_info[\"Environment\"].replace(\" \", \"_\")}.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nüìä Visualizations saved!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mhdk_1FYtXdB"
      },
      "outputs": [],
      "source": [
        "# Cell 8: Dimensionality Reduction Performance Test\n",
        "print(\"\\nüîç TESTING DIMENSIONALITY REDUCTION ALGORITHMS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Use medium dataset for dimensionality reduction\n",
        "X_test, y_test, _ = datasets['medium']\n",
        "X_test_scaled = StandardScaler().fit_transform(X_test)\n",
        "\n",
        "# Test PCA\n",
        "print(\"\\n‚è±Ô∏è  Running PCA...\")\n",
        "start_pca = time.time()\n",
        "pca = PCA(n_components=2, random_state=42)\n",
        "X_pca = pca.fit_transform(X_test_scaled)\n",
        "pca_time = time.time() - start_pca\n",
        "print(f\"   PCA completed in {pca_time:.4f} seconds\")\n",
        "print(f\"   Explained variance ratio: {pca.explained_variance_ratio_.sum():.4f}\")\n",
        "\n",
        "# Test t-SNE (on subset due to computational cost)\n",
        "print(\"\\n‚è±Ô∏è  Running t-SNE (on 10K sample subset)...\")\n",
        "X_tsne_subset = X_test_scaled[:10000]\n",
        "y_tsne_subset = y_test[:10000]\n",
        "start_tsne = time.time()\n",
        "tsne = TSNE(n_components=2, random_state=42, n_jobs=-1)\n",
        "X_tsne = tsne.fit_transform(X_tsne_subset)\n",
        "tsne_time = time.time() - start_tsne\n",
        "print(f\"   t-SNE completed in {tsne_time:.4f} seconds\")\n",
        "\n",
        "# Visualize\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# PCA visualization\n",
        "ax1 = axes[0]\n",
        "scatter1 = ax1.scatter(X_pca[:, 0], X_pca[:, 1], c=y_test, cmap='viridis',\n",
        "                       alpha=0.5, s=10)\n",
        "ax1.set_title(f'PCA Projection (Time: {pca_time:.2f}s)', fontsize=14, fontweight='bold')\n",
        "ax1.set_xlabel('First Principal Component')\n",
        "ax1.set_ylabel('Second Principal Component')\n",
        "plt.colorbar(scatter1, ax=ax1)\n",
        "\n",
        "# t-SNE visualization\n",
        "ax2 = axes[1]\n",
        "scatter2 = ax2.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y_tsne_subset, cmap='viridis',\n",
        "                       alpha=0.5, s=10)\n",
        "ax2.set_title(f't-SNE Projection (Time: {tsne_time:.2f}s)', fontsize=14, fontweight='bold')\n",
        "ax2.set_xlabel('t-SNE Component 1')\n",
        "ax2.set_ylabel('t-SNE Component 2')\n",
        "plt.colorbar(scatter2, ax=ax2)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f'dimensionality_reduction_{system_info[\"Environment\"].replace(\" \", \"_\")}.png',\n",
        "            dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3B_RfoAtXdB"
      },
      "source": [
        "## Key Learning Points for Students\n",
        "\n",
        "### 1Ô∏è‚É£ Computational Resources Matter\n",
        "- Different environments provide different computational capabilities\n",
        "- RAM and CPU cores directly impact algorithm performance\n",
        "- NRP resources may provide advantages for large-scale ML tasks\n",
        "\n",
        "### 2Ô∏è‚É£ Algorithm Complexity\n",
        "- **K-Means**: O(n√ók√ói) - Generally fastest, scales well\n",
        "- **DBSCAN**: O(n¬≤) in worst case - Good for noise, slower on large datasets\n",
        "- **Hierarchical**: O(n¬≤√ólog(n)) - Most computationally expensive\n",
        "\n",
        "### 3Ô∏è‚É£ Quality Metrics\n",
        "- **Silhouette Score**: Measures cluster cohesion (range: -1 to 1, higher is better)\n",
        "- **Davies-Bouldin Index**: Measures cluster separation (lower is better)\n",
        "- **Calinski-Harabasz**: Variance ratio criterion (higher is better)\n",
        "\n",
        "### 4Ô∏è‚É£ Practical Implications\n",
        "- Choose algorithms based on dataset size and available resources\n",
        "- Consider quality vs. speed trade-offs\n",
        "- Infrastructure choice affects experimental capabilities\n",
        "\n",
        "### 5Ô∏è‚É£ Next Steps for Exploration\n",
        "- Try different clustering parameters (k values, eps, linkage methods)\n",
        "- Experiment with feature engineering and preprocessing\n",
        "- Compare results between NRP and Google Colab environments\n",
        "- Explore other unsupervised methods (GMM, Spectral Clustering)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m3qqdRoYtXdC"
      },
      "outputs": [],
      "source": [
        "# Cell 9: Generate Summary Report\n",
        "from datetime import datetime\n",
        "\n",
        "report = f\"\"\"\n",
        "UNSUPERVISED LEARNING PERFORMANCE BENCHMARK REPORT\n",
        "Florida A&M University - Computer & Information Sciences Department\n",
        "Instructor: Dr. Yohn\n",
        "Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
        "\n",
        "{'=' * 80}\n",
        "SYSTEM CONFIGURATION\n",
        "{'=' * 80}\n",
        "Environment: {system_info['Environment']}\n",
        "CPU Cores (Logical): {system_info['CPU Cores (Logical)']}\n",
        "Total RAM: {system_info['Total RAM (GB)']} GB\n",
        "Available RAM: {system_info['Available RAM (GB)']} GB\n",
        "Python Version: {system_info['Python Version']}\n",
        "\n",
        "{'=' * 80}\n",
        "PERFORMANCE SUMMARY\n",
        "{'=' * 80}\n",
        "\n",
        "{results_df.to_string(index=False)}\n",
        "\n",
        "{'=' * 80}\n",
        "FASTEST ALGORITHMS BY DATASET SIZE\n",
        "{'=' * 80}\n",
        "\"\"\"\n",
        "\n",
        "for dataset_name in ['small', 'medium', 'large']:\n",
        "    dataset_results = results_df[results_df['Dataset'] == dataset_name]\n",
        "    fastest = dataset_results.loc[dataset_results['Execution Time (s)'].idxmin()]\n",
        "    report += f\"\\n{dataset_name.upper()}: {fastest['Algorithm']} ({fastest['Execution Time (s)']}s)\"\n",
        "\n",
        "report += f\"\"\"\n",
        "\n",
        "{'=' * 80}\n",
        "RECOMMENDATIONS FOR STUDENTS\n",
        "{'=' * 80}\n",
        "1. For exploratory analysis: Use K-Means on smaller datasets\n",
        "2. For production systems: Consider computational resources carefully\n",
        "3. For best quality: Compare multiple algorithms and validate with metrics\n",
        "4. When using NRP: Leverage increased resources for larger experiments\n",
        "\n",
        "Report generated on {system_info['Environment']}\n",
        "Compare this with results from Google Colab to see resource differences!\n",
        "{'=' * 80}\n",
        "\"\"\"\n",
        "\n",
        "# Save report\n",
        "report_filename = f\"benchmark_report_{system_info['Environment'].replace(' ', '_')}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\"\n",
        "with open(report_filename, 'w') as f:\n",
        "    f.write(report)\n",
        "\n",
        "print(report)\n",
        "print(f\"\\nüíæ Full report saved to: {report_filename}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lew6ieUvtXdC"
      },
      "source": [
        "## Interactive Comparison Tool\n",
        "\n",
        "### To Compare NRP vs Google Colab Performance:\n",
        "\n",
        "1. Run this entire notebook on **JupyterHub NRP** (you're doing this now!)\n",
        "2. Download the generated CSV file: `clustering_benchmark_*.csv`\n",
        "3. Open this same notebook in **Google Colab**\n",
        "4. Run all cells in Google Colab\n",
        "5. Download the Colab CSV file\n",
        "6. Upload both CSV files and run the comparison cell below\n",
        "\n",
        "This hands-on comparison will help you understand:\n",
        "- How computational resources affect ML performance\n",
        "- When to use cloud resources vs local/NRP resources\n",
        "- Real-world considerations for deploying ML models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gzyqa3fwtXdC"
      },
      "outputs": [],
      "source": [
        "# Cell 10: Interactive Comparison Function (Optional)\n",
        "def compare_environments(nrp_csv, colab_csv):\n",
        "    \"\"\"\n",
        "    Compare performance between NRP and Google Colab\n",
        "\n",
        "    Usage: compare_environments('nrp_results.csv', 'colab_results.csv')\n",
        "    \"\"\"\n",
        "    nrp_df = pd.read_csv(nrp_csv)\n",
        "    colab_df = pd.read_csv(colab_csv)\n",
        "\n",
        "    nrp_df['Environment'] = 'NRP'\n",
        "    colab_df['Environment'] = 'Colab'\n",
        "\n",
        "    combined = pd.concat([nrp_df, colab_df])\n",
        "\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "    # Execution time comparison\n",
        "    ax1 = axes[0, 0]\n",
        "    combined.groupby(['Dataset', 'Environment'])['Execution Time (s)'].mean().unstack().plot(\n",
        "        kind='bar', ax=ax1, colormap='Set2')\n",
        "    ax1.set_title('Average Execution Time: NRP vs Colab', fontsize=14, fontweight='bold')\n",
        "    ax1.set_ylabel('Time (seconds)')\n",
        "    ax1.legend(title='Environment')\n",
        "\n",
        "    # Memory comparison\n",
        "    ax2 = axes[0, 1]\n",
        "    combined.groupby(['Dataset', 'Environment'])['Memory Used (MB)'].mean().unstack().plot(\n",
        "        kind='bar', ax=ax2, colormap='Set2')\n",
        "    ax2.set_title('Average Memory Usage: NRP vs Colab', fontsize=14, fontweight='bold')\n",
        "    ax2.set_ylabel('Memory (MB)')\n",
        "    ax2.legend(title='Environment')\n",
        "\n",
        "    # Quality comparison\n",
        "    ax3 = axes[1, 0]\n",
        "    combined.groupby(['Dataset', 'Environment'])['Silhouette Score'].mean().unstack().plot(\n",
        "        kind='bar', ax=ax3, colormap='Set2')\n",
        "    ax3.set_title('Average Clustering Quality: NRP vs Colab', fontsize=14, fontweight='bold')\n",
        "    ax3.set_ylabel('Silhouette Score')\n",
        "    ax3.legend(title='Environment')\n",
        "\n",
        "    # Speedup factor\n",
        "    ax4 = axes[1, 1]\n",
        "    speedup_data = []\n",
        "    for dataset in combined['Dataset'].unique():\n",
        "        for algo in combined['Algorithm'].unique():\n",
        "            nrp_time = nrp_df[(nrp_df['Dataset']==dataset) & (nrp_df['Algorithm']==algo)]['Execution Time (s)'].values\n",
        "            colab_time = colab_df[(colab_df['Dataset']==dataset) & (colab_df['Algorithm']==algo)]['Execution Time (s)'].values\n",
        "            if len(nrp_time) > 0 and len(colab_time) > 0 and nrp_time[0] > 0:\n",
        "                speedup = colab_time[0] / nrp_time[0]\n",
        "                speedup_data.append({'Dataset': dataset, 'Algorithm': algo, 'Speedup': speedup})\n",
        "\n",
        "    if speedup_data:\n",
        "        speedup_df = pd.DataFrame(speedup_data)\n",
        "        speedup_pivot = speedup_df.pivot(index='Algorithm', columns='Dataset', values='Speedup')\n",
        "        speedup_pivot.plot(kind='bar', ax=ax4, colormap='RdYlGn')\n",
        "        ax4.set_title('Speedup Factor (Colab Time / NRP Time)', fontsize=14, fontweight='bold')\n",
        "        ax4.set_ylabel('Speedup (>1 means NRP faster)')\n",
        "        ax4.axhline(y=1, color='black', linestyle='--', label='Equal performance')\n",
        "        ax4.legend(title='Dataset')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('nrp_vs_colab_comparison.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    print(\"\\nüìä ENVIRONMENT COMPARISON SUMMARY\")\n",
        "    print(\"=\" * 80)\n",
        "    print(combined.groupby('Environment')[['Execution Time (s)', 'Memory Used (MB)',\n",
        "                                            'Silhouette Score']].mean())\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "# Uncomment and run when you have both CSV files:\n",
        "# compare_environments('clustering_benchmark_JupyterHub_NRP_(or_Local).csv',\n",
        "#                      'clustering_benchmark_Google_Colab.csv')\n",
        "\n",
        "print(\"\\n‚úÖ Notebook Complete!\")\n",
        "print(\"üìä Review your results and compare with Google Colab when ready.\")\n",
        "print(\"üí° Discussion: What differences did you observe? Why might they exist?\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}